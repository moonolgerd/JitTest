# JiTTest — Requirements

## Overview

Implement a **Catching Just-in-Time Test (JiTTest)** system for the AspireWithDapr codebase, inspired by [Meta's Catching JiTTest research](https://engineering.fb.com/2026/02/11/developer-tools/the-death-of-traditional-testing-agentic-development-jit-testing-revival/) ([paper](https://arxiv.org/abs/2601.22832)). JiTTests are ephemeral, per-change tests automatically generated by an LLM to catch regressions **before code lands in production** — they are never checked into source control.

## Background

Traditional test suites are static, require manual authoring, and ongoing maintenance. As agentic development accelerates code velocity, this approach doesn't scale. Meta's Catching JiTTest approach reimagines testing:

- Tests are generated **on-the-fly** for each code change
- Tests are designed to **fail** (catch regressions), not merely pass
- LLM-generated **realistic mutants** replace rule-based mutation operators
- Tests are **ephemeral** — never stored in the codebase, eliminating maintenance burden
- **Assessors** (rule-based + LLM-based) filter false positives to reduce developer drag

## Functional Requirements

### FR-1: Diff Extraction

- **FR-1.1**: Extract code changes from the local git repository (staged, unstaged, or branch comparison).
- **FR-1.2**: Parse diffs into structured data: file path, changed lines, before/after hunks, surrounding context (±20 lines).
- **FR-1.3**: Support configurable diff sources: `staged`, `uncommitted`, `branch:<target>`.
- **FR-1.4**: Filter diffs by configurable glob patterns to include/exclude files.

### FR-2: Intent Inference

- **FR-2.1**: Send the extracted diff and file context to a local LLM (Ollama).
- **FR-2.2**: Produce a structured intent summary describing: what changed, expected behavior modification, and potential risk areas.

### FR-3: Mutant Generation

- **FR-3.1**: Generate 3–5 realistic, domain-specific mutants per code change using the LLM.
- **FR-3.2**: Mutants must be contextual — based on the inferred intent and the actual code semantics, not generic operator flips.
- **FR-3.3**: Each mutant must be representable as a reversible code patch.
- **FR-3.4**: Avoid equivalent mutants (syntactically different but semantically identical) using LLM awareness.

### FR-4: Test Generation

- **FR-4.1**: For each mutant, generate a self-contained xUnit test that:
  - **Passes** against the original code
  - **Fails** against the mutated code
- **FR-4.2**: Tests must reference actual project types (`WeatherForecast`, `WeatherUtilities`, etc.).
- **FR-4.3**: Validate generated tests compile using Roslyn in-memory compilation.
- **FR-4.4**: Retry failed compilations by feeding compiler errors back to the LLM (up to 2 retries).

### FR-5: Test Execution

- **FR-5.1**: Execute each generated test against the original code; it must pass.
- **FR-5.2**: Apply the mutant patch, re-execute; the test must fail.
- **FR-5.3**: Only tests satisfying both conditions qualify as **candidate catches**.
- **FR-5.4**: Source files are never directly mutated — mutations are applied to isolated shadow copies of the target project.
- **FR-5.5**: Multiple test executions may run in parallel using independent shadow copies, bounded by `max-parallel`.

### FR-6: False Positive Assessment

- **FR-6.1**: Apply a rule-based assessor to reject trivial catches (constants-only assertions, infrastructure code, compiler-generated code).
- **FR-6.2**: Apply an LLM-based assessor to evaluate whether the mutant represents a plausible real bug; use binary yes/no + reasoning format.
- **FR-6.3**: Filter candidate catches below a configurable confidence threshold.

### FR-7: Reporting

- **FR-7.1**: Output a human-readable report (console + optional markdown file) listing:
  - Code change analyzed (file, lines)
  - Each candidate catch: mutant description, generated test code, confidence assessment
  - Summary: total catches, files affected
- **FR-7.2**: Exit with non-zero code if candidate catches are found (for CI integration).

### FR-8: Configuration

- **FR-8.1**: Support a `jittest-config.json` file at solution root with all configurable parameters.
- **FR-8.2**: Allow CLI overrides for all configuration values.
- **FR-8.3**: Configurable parameters: Ollama endpoint, model name, mutate targets (globs), exclusions, max mutants per change, max retries, max parallel, diff source, confidence threshold.

## Non-Functional Requirements

### NFR-1: Local-First Execution

- All LLM inference runs on a local Ollama instance — no cloud dependencies.
- Target model: `qwen2.5-coder:32b-instruct-q4_K_M` (fits in 32GB RAM).
- Must work offline after model is pulled.

### NFR-2: Performance

- Full pipeline for a single-file change should complete in < 5 minutes on local hardware.
- Diff extraction and Roslyn compilation should be < 5 seconds each.
- LLM calls dominate runtime; stages 4–6 (test generation, execution, assessment) run with configurable parallelism (`max-parallel`, default: 3) to overlap independent LLM calls and `dotnet test` invocations.
- Test execution uses shadow-copy isolation (per-execution temp directory copies of the target project), enabling safe parallel `dotnet test` without file-level locking.
- Stage timing telemetry is printed after each run to support data-driven tuning.

### NFR-3: Compatibility

- .NET 10.0 target framework (matches the existing solution).
- Must work on Windows (primary development environment).
- Use `Microsoft.Extensions.AI.OpenAI` / `IChatClient` abstraction — identical to the existing Web project — so the LLM backend can be swapped to cloud by changing one URL.

### NFR-4: No Codebase Pollution

- Generated tests are ephemeral — written to a `temp/` directory and cleaned up after execution.
- No test projects or test files are checked into source control.
- The JiTTest tool itself is a standalone console app/dotnet tool.

### NFR-5: Extensibility

- The pipeline stages (diff → intent → mutants → tests → execute → assess → report) should be modular, allowing individual stages to be replaced or extended.

## Target Codebase Scope

The following files contain the highest-value mutation targets:

| File | Testable Logic | Mutant Density |
|------|---------------|----------------|
| `AspireWithDapr.Shared/WeatherUtilities.cs` | 4 pure static methods, 10+ branch conditions, `StringComparison` | High |
| `AspireWithDapr.Shared/WeatherForecast.cs` | `TemperatureF` computed property (arithmetic) | Medium |
| `AspireWithDapr.ApiService/WeatherActor.cs` | Duplicate detection, state management, event publishing | High |
| `AspireWithDapr.ApiService/Query.cs` | Actor proxy delegation | Low |
| `AspireWithDapr.ApiService/Mutation.cs` | Actor proxy delegation | Low |
| `AspireWithDapr.Publisher/PublisherHostedService.cs` | Weather data generation, pub/sub publishing | Medium |

## Glossary

| Term | Definition |
|------|-----------|
| **JiTTest** | Just-in-Time Test — ephemeral test generated per code change, never checked in |
| **Catching Test** | A test designed to fail, surfacing regressions in new code |
| **Mutant** | A version of the code with a fault deliberately inserted |
| **Candidate Catch** | A test that passes on original code and fails on mutated code |
| **Assessor** | A filter (rule-based or LLM-based) that evaluates whether a candidate catch is a true positive |
| **Equivalent Mutant** | A mutant syntactically different but semantically identical to the original — a false positive source |

## References

- [Meta: The Death of Traditional Testing (Feb 2026)](https://engineering.fb.com/2026/02/11/developer-tools/the-death-of-traditional-testing-agentic-development-jit-testing-revival/)
- [arXiv: Just-in-Time Catching Test Generation at Meta (Jan 2026)](https://arxiv.org/abs/2601.22832)
- [Meta: LLMs Are the Key to Mutation Testing (Sep 2025)](https://engineering.fb.com/2025/09/30/security/llms-are-the-key-to-mutation-testing-and-better-compliance/)
- [Meta: Revolutionizing Software Testing — ACH (Feb 2025)](https://engineering.fb.com/2025/02/05/security/revolutionizing-software-testing-llm-powered-bug-catchers-meta-ach/)
- [arXiv: Mutation-Guided LLM-based Test Generation at Meta](https://arxiv.org/pdf/2501.12862)
